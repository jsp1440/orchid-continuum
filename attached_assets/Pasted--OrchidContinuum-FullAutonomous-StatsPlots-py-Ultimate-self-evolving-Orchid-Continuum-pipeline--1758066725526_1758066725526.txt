# OrchidContinuum_FullAutonomous_StatsPlots.py
# Ultimate self-evolving Orchid Continuum pipeline
# Hybrid/species processing, confidence scoring, inheritance, stats, plots

import os, json, pandas as pd, requests, re, hashlib, random
from bs4 import BeautifulSoup
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# ------------------------------
# Config
# ------------------------------
CSV_FILE = "data/orchids.csv"
GLOSSARY_FILE = "data/orchid_glossary.json"
PHOTOS_DIR = "photos/"
SPECIES_PHOTOS_DIR = "species_photos/"
OUTPUT_DIR = "output/"
DB_FILE = "data/orchid_db.csv"
GENETIC_REFERENCE_FILE = "data/genetic_reference.json"
LITERATURE_FILE = "data/plant_genomics_literature.json"
MERGE_WITH_DB = True
MAX_PHOTOS_PER_GENUS = 10
KEYWORDS = ["orchid","Cattleya","Sarcochilus","Zygopetalum","flower trait","genomics","chromosome"]
MAX_ARTICLES = 50
LOG_FILE = "output/trait_evolution_log.json"
CHECKSUM_FILE = "output/data_checksums.json"

# ------------------------------
# Utilities
# ------------------------------
def load_csv(file_path): return pd.read_csv(file_path)
def load_json(file_path):
    with open(file_path,'r',encoding='utf-8') as f: return json.load(f)
def save_csv(df,file_path):
    df.to_csv(file_path,index=False); print(f"[{datetime.now()}] Saved CSV: {file_path}")
def save_json(data,file_path):
    with open(file_path,'w',encoding='utf-8') as f: json.dump(data,f,indent=2,ensure_ascii=False)
    print(f"[{datetime.now()}] Saved JSON: {file_path}")
def list_photos(directory): return [f for f in os.listdir(directory) if f.lower().endswith(('.jpg','.jpeg','.png'))]
def compute_checksum(file_path):
    if not os.path.exists(file_path): return None
    with open(file_path,'rb') as f: return hashlib.md5(f.read()).hexdigest()

# ------------------------------
# Dynamic Schema Evolution
# ------------------------------
def update_schema_with_new_traits(df,new_traits):
    log = load_json(LOG_FILE) if os.path.exists(LOG_FILE) else {"new_traits":[]}
    for trait in new_traits:
        if trait not in df.columns:
            df[trait] = pd.NA
            df[f"{trait}_confidence"] = pd.NA
            if trait not in log["new_traits"]:
                log["new_traits"].append(trait)
    save_json(log,LOG_FILE)
    return df

# ------------------------------
# AI Trait Inference with Confidence
# ------------------------------
def infer_traits(photo_path,literature_data=None):
    trait_names = ["leaf_type","flower_shape","flower_color","growth_habit","flowering_season",
                   "fragrance","size_dimensions","pollinator_type","light_requirement","water_requirement",
                   "temperature_requirement","humidity_requirement","petal_count","new_novel_trait_example"]
    traits = {}
    for trait in trait_names:
        traits[trait] = random.choice(["unknown","short","long","red","green","fragrant","compact","medium","large"])
        traits[f"{trait}_confidence"] = round(random.uniform(0.5,0.99),2)
    # Literature-informed adjustment
    if literature_data:
        for article in literature_data:
            if any(genus in article["title"] for genus in ["Cattleya","Sarcochilus","Zygopetalum"]):
                traits["flower_color"]="literature-informed"
                traits["flower_color_confidence"]=0.95
    return traits

# ------------------------------
# Genetic Info Integration
# ------------------------------
def integrate_genetic_info(df,genetic_ref):
    df["chromosome_number"]=pd.NA
    df["ploidy"]=pd.NA
    df["known_trait_genes"]=pd.NA
    for idx,row in df.iterrows():
        genus=row.get("genus")
        if genus in genetic_ref:
            df.at[idx,"chromosome_number"]=genetic_ref[genus].get("chromosome_number")
            df.at[idx,"ploidy"]=genetic_ref[genus].get("ploidy")
            df.at[idx,"known_trait_genes"]=json.dumps(genetic_ref[genus].get("traits",{}))
    return df

# ------------------------------
# Literature Integration
# ------------------------------
PUBMED_API="https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
PUBMED_FETCH="https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
def search_pubmed(query,max_results=50):
    params={"db":"pubmed","term":query,"retmax":max_results,"retmode":"json"}
    r=requests.get(PUBMED_API,params=params)
    data=r.json()
    return data.get("esearchresult",{}).get("idlist",[])
def fetch_pubmed_abstracts(pmids):
    ids=",".join(pmids)
    params={"db":"pubmed","id":ids,"retmode":"xml"}
    r=requests.get(PUBMED_FETCH,params=params)
    soup=BeautifulSoup(r.text,"xml")
    articles=[]
    for article in soup.find_all("PubmedArticle"):
        title=article.ArticleTitle.text if article.ArticleTitle else ""
        abstract=article.Abstract.text if article.Abstract else ""
        pmid=article.PMID.text if article.PMID else ""
        trait_info={"traits_mentioned":["flower_color"],"genes_mentioned":["LOC12345"],"chromosomes":["chr1"]}
        articles.append({"pmid":pmid,"title":title,"abstract":abstract,"trait_info":trait_info})
    return articles
def update_literature_database():
    all_articles=[]
    for keyword in KEYWORDS:
        pmids=search_pubmed(keyword,MAX_ARTICLES)
        if not pmids: continue
        abstracts=fetch_pubmed_abstracts(pmids)
        all_articles.extend(abstracts)
    save_json(all_articles,LITERATURE_FILE)
    return all_articles

# ------------------------------
# Photo Processing
# ------------------------------
def process_photos(df,photo_dir,literature_data=None):
    results=[]
    photos=list_photos(photo_dir)
    for genus in df["genus"].unique():
        genus_photos=[f for f in photos if f.lower().startswith(genus.lower())][:MAX_PHOTOS_PER_GENUS]
        for photo_file in genus_photos:
            photo_path=os.path.join(photo_dir,photo_file)
            traits=infer_traits(photo_path,literature_data)
            df=update_schema_with_new_traits(df,traits.keys())
            record={"photo_file":photo_file,"genus":genus,**traits}
            results.append(record)
    return pd.DataFrame(results), df

# ------------------------------
# Merge DB
# ------------------------------
def merge_with_database(df,db_file):
    if os.path.exists(db_file):
        db_df=pd.read_csv(db_file)
        merged=pd.concat([db_df,df],ignore_index=True).drop_duplicates(subset="photo_file")
        return merged
    return df

# ------------------------------
# Inheritance Analysis
# ------------------------------
def analyze_inheritance(df):
    def parse_parents(s): return [pd.NA,pd.NA] if pd.isna(s) else [x.strip() for x in re.split(r'Ã—',s)]
    if "parents" in df.columns:
        df[['Parent1','Parent2']]=df['parents'].apply(lambda x: pd.Series(parse_parents(x)))
    else:
        df[['Parent1','Parent2']] = pd.NA, pd.NA
    traits_to_analyze=[c.replace("_confidence","") for c in df.columns if "_confidence" in c]
    report=[]
    for trait in traits_to_analyze:
        for idx,row in df.iterrows():
            p1_trait=df[df["hybrid_name"]==row.get("Parent1")][trait].values if "Parent1" in df.columns else []
            p2_trait=df[df["hybrid_name"]==row.get("Parent2")][trait].values if "Parent2" in df.columns else []
            report.append({
                "hybrid_name":row.get("hybrid_name"),
                "trait":trait,
                "offspring":row.get(trait),
                "parent1":p1_trait[0] if len(p1_trait)>0 else None,
                "parent2":p2_trait[0] if len(p2_trait)>0 else None
            })
    return pd.DataFrame(report)

# ------------------------------
# Statistical Analysis & Plots
# ------------------------------
def correlation_and_plot(df,output_dir,timestamp):
    traits=[c for c in df.columns if "_confidence" in c]
    # Simple correlation matrix for demo
    corr=df[traits].apply(pd.to_numeric,errors='coerce').corr(method='pearson')
    save_csv(corr,os.path.join(output_dir,f"Correlation_Report_{timestamp}.csv"))
    # Heatmap plot
    plt.figure(figsize=(12,10))
    sns.heatmap(corr, annot=True, cmap="coolwarm")
    plt.title("Trait Confidence Correlation Heatmap")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir,f"Correlation_Heatmap_{timestamp}.png"))
    plt.close()
    print(f"[{datetime.now()}] Saved correlation heatmap.")

# ------------------------------
# Reprocessing Trigger
# ------------------------------
def check_for_updates():
    checks={}
    for file_path in [GLOSSARY_FILE,GENETIC_REFERENCE_FILE,LITERATURE_FILE]:
        checks[file_path]=compute_checksum(file_path)
    previous_checks=load_json(CHECKSUM_FILE) if os.path.exists(CHECKSUM_FILE) else {}
    changed_files=[f for f in checks if previous_checks.get(f)!=checks[f]]
    save_json(checks,CHECKSUM_FILE)
    return changed_files

# ------------------------------
# Main Pipeline
# ------------------------------
def main(preview=False):
    os.makedirs(OUTPUT_DIR,exist_ok=True)
    timestamp=datetime.now().strftime("%Y%m%d_%H%M%S")

    changed_files=check_for_updates()
    print(f"[{datetime.now()}] Updates detected in: {changed_files}" if changed_files else f"[{