"""
Advanced orchid scrapers for major orchid websites with rich metadata extraction
Targeting ecuagenera.com, Andys orchids, and Jays internet encyclopedia
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from models import OrchidRecord, db
from datetime import datetime
import time
import re
import logging
from PIL import Image
from PIL.ExifTags import TAGS, GPSTAGS
import io

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EcuageneraScraper:
    """Scraper for Ecuagenera - major orchid vendor with extensive catalog"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; OrchidBot/1.0; Educational/Research)'
        })
        self.base_url = "https://ecuagenera.com"
        
    def scrape_orchid_catalog(self, max_pages=10):
        """Scrape Ecuagenera's orchid catalog"""
        logger.info(f"üå∫ Starting Ecuagenera scraper - targeting {max_pages} pages")
        
        results = {'processed': 0, 'errors': 0, 'skipped': 0}
        
        # Main orchid categories on Ecuagenera
        categories = [
            '/collections/cattleya',
            '/collections/dendrobium', 
            '/collections/bulbophyllum',
            '/collections/masdevallia',
            '/collections/dracula',
            '/collections/phalaenopsis',
            '/collections/oncidium-alliance',
            '/collections/paphiopedilum',
            '/collections/epidendrum',
            '/collections/lycaste'
        ]
        
        for category in categories:
            logger.info(f"üìÇ Processing category: {category}")
            
            try:
                page_results = self.scrape_category_page(category, max_pages=max_pages//len(categories))
                results['processed'] += page_results['processed']
                results['errors'] += page_results['errors']
                results['skipped'] += page_results['skipped']
                
                time.sleep(2)  # Be respectful
                
            except Exception as e:
                logger.error(f"‚ùå Error processing category {category}: {e}")
                results['errors'] += 1
        
        return results
    
    def scrape_category_page(self, category_path, max_pages=3):
        """Scrape a specific category page"""
        results = {'processed': 0, 'errors': 0, 'skipped': 0}
        
        for page in range(1, max_pages + 1):
            try:
                url = f"{self.base_url}{category_path}?page={page}"
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Find product links
                product_links = soup.find_all('a', class_='product-item-link')
                
                if not product_links:
                    break  # No more products
                
                for link in product_links:
                    try:
                        product_url = urljoin(self.base_url, link.get('href'))
                        orchid_data = self.scrape_orchid_detail(product_url)
                        
                        if orchid_data:
                            self.save_orchid_record(orchid_data)
                            results['processed'] += 1
                        else:
                            results['skipped'] += 1
                            
                    except Exception as e:
                        logger.error(f"Error processing product: {e}")
                        results['errors'] += 1
                
                time.sleep(1)  # Rate limiting
                
            except Exception as e:
                logger.error(f"Error scraping page {page}: {e}")
                results['errors'] += 1
        
        return results
    
    def scrape_orchid_detail(self, product_url):
        """Extract detailed orchid information from product page"""
        try:
            response = self.session.get(product_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract orchid name
            title_elem = soup.find('h1', class_='product-title')
            if not title_elem:
                return None
                
            full_name = title_elem.text.strip()
            
            # Parse scientific name
            genus, species = self.parse_scientific_name(full_name)
            
            # Extract image
            img_elem = soup.find('img', class_='product-image')
            image_url = None
            if img_elem:
                image_url = urljoin(self.base_url, img_elem.get('src'))
            
            # Extract metadata from description
            description_elem = soup.find('div', class_='product-description')
            metadata = self.extract_metadata_from_description(description_elem)
            
            # Extract price and availability info
            price_elem = soup.find('span', class_='price')
            price = price_elem.text.strip() if price_elem else None
            
            orchid_data = {
                'display_name': full_name,
                'scientific_name': full_name,
                'genus': genus,
                'species': species,
                'image_url': image_url,
                'region': metadata.get('origin', 'Ecuador'),  # Ecuagenera is based in Ecuador
                'native_habitat': metadata.get('habitat'),
                'cultural_notes': metadata.get('care_notes'),
                'photographer': 'Ecuagenera',
                'image_source': 'Ecuagenera Catalog',
                'ingestion_source': 'ecuagenera_scraper'
            }
            
            return orchid_data
            
        except Exception as e:
            logger.error(f"Error scraping orchid detail from {product_url}: {e}")
            return None
    
    def parse_scientific_name(self, name):
        """Parse genus and species from orchid name"""
        # Remove common prefixes and clean up
        name = re.sub(r'^(Orchid\s+)?', '', name, flags=re.IGNORECASE)
        name = re.sub(r'\s*\([^)]*\)\s*', ' ', name)  # Remove parenthetical info
        
        parts = name.split()
        if len(parts) >= 2:
            return parts[0], parts[1]
        elif len(parts) == 1:
            return parts[0], None
        return None, None
    
    def extract_metadata_from_description(self, description_elem):
        """Extract metadata from product description"""
        metadata = {}
        
        if not description_elem:
            return metadata
            
        text = description_elem.get_text()
        
        # Look for origin/native information
        origin_match = re.search(r'(native|from|origin|found)\s+(?:in\s+)?([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)', text, re.IGNORECASE)
        if origin_match:
            metadata['origin'] = origin_match.group(2)
        
        # Look for habitat information
        habitat_match = re.search(r'(habitat|grows?|found)\s+(?:in\s+)?([^.]+)', text, re.IGNORECASE)
        if habitat_match:
            metadata['habitat'] = habitat_match.group(2).strip()
        
        # Look for care notes
        if 'care' in text.lower() or 'temperature' in text.lower() or 'humidity' in text.lower():
            metadata['care_notes'] = text[:500]  # First 500 chars
        
        return metadata
    
    def save_orchid_record(self, orchid_data):
        """Save orchid record to database"""
        try:
            # Check if already exists
            existing = OrchidRecord.query.filter_by(
                scientific_name=orchid_data['scientific_name'],
                ingestion_source=orchid_data['ingestion_source']
            ).first()
            
            if existing:
                return existing
            
            # Create new record
            record = OrchidRecord(**orchid_data)
            db.session.add(record)
            db.session.commit()
            
            logger.info(f"‚úÖ Added {orchid_data['display_name']} from Ecuagenera")
            return record
            
        except Exception as e:
            db.session.rollback()
            logger.error(f"Error saving orchid record: {e}")
            return None


class AndysOrchidsScraper:
    """Scraper for Andy's Orchids - specialized nursery with rare species"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; OrchidBot/1.0; Educational/Research)'
        })
        self.base_url = "https://andysorchids.com"
        
    def scrape_orchid_catalog(self, max_pages=15):
        """Scrape Andy's Orchids catalog"""
        logger.info(f"üå∫ Starting Andy's Orchids scraper - targeting {max_pages} pages")
        
        results = {'processed': 0, 'errors': 0, 'skipped': 0}
        
        # Andy's has a comprehensive catalog page
        try:
            catalog_results = self.scrape_catalog_pages(max_pages)
            results['processed'] += catalog_results['processed']
            results['errors'] += catalog_results['errors']
            results['skipped'] += catalog_results['skipped']
            
        except Exception as e:
            logger.error(f"‚ùå Error scraping Andy's catalog: {e}")
            results['errors'] += 1
        
        return results
    
    def scrape_catalog_pages(self, max_pages):
        """Scrape catalog pages from Andy's Orchids"""
        results = {'processed': 0, 'errors': 0, 'skipped': 0}
        
        # Different sections of Andy's catalog
        sections = [
            '/Orchid_Species_for_Sale.htm',
            '/Masdevallia_for_Sale.htm', 
            '/Pleurothallis_for_Sale.htm',
            '/Bulbophyllum_for_Sale.htm'
        ]
        
        for section in sections:
            try:
                url = f"{self.base_url}{section}"
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Andy's uses tables for orchid listings
                orchid_rows = soup.find_all('tr')
                
                for row in orchid_rows:
                    try:
                        orchid_data = self.parse_orchid_row(row)
                        if orchid_data:
                            self.save_orchid_record(orchid_data)
                            results['processed'] += 1
                        else:
                            results['skipped'] += 1
                            
                    except Exception as e:
                        logger.error(f"Error processing row: {e}")
                        results['errors'] += 1
                
                time.sleep(2)  # Be respectful
                
            except Exception as e:
                logger.error(f"Error scraping section {section}: {e}")
                results['errors'] += 1
        
        return results
    
    def parse_orchid_row(self, row):
        """Parse orchid information from table row"""
        try:
            cells = row.find_all('td')
            if len(cells) < 3:
                return None
            
            # Andy's typically has: Name | Description | Price
            name_cell = cells[0]
            desc_cell = cells[1] if len(cells) > 1 else None
            
            # Extract orchid name
            name_text = name_cell.get_text().strip()
            if not name_text or len(name_text) < 3:
                return None
            
            # Parse scientific name
            genus, species = self.parse_scientific_name(name_text)
            if not genus:
                return None
            
            # Extract image if present
            img_elem = name_cell.find('img') or (desc_cell.find('img') if desc_cell else None)
            image_url = None
            if img_elem:
                image_url = urljoin(self.base_url, img_elem.get('src'))
            
            # Extract description and metadata
            description = desc_cell.get_text().strip() if desc_cell else ""
            metadata = self.extract_metadata_from_description(description)
            
            orchid_data = {
                'display_name': name_text,
                'scientific_name': name_text,
                'genus': genus,
                'species': species,
                'image_url': image_url,
                'region': metadata.get('origin', 'California'),  # Andy's is in California
                'native_habitat': metadata.get('habitat'),
                'cultural_notes': description[:500] if description else None,
                'photographer': "Andy's Orchids",
                'image_source': "Andy's Orchids Catalog",
                'ingestion_source': 'andys_orchids_scraper'
            }
            
            return orchid_data
            
        except Exception as e:
            logger.error(f"Error parsing orchid row: {e}")
            return None
    
    def parse_scientific_name(self, name):
        """Parse genus and species from orchid name"""
        # Clean up the name
        name = re.sub(r'\s*[\(\[].*?[\)\]]\s*', ' ', name)  # Remove parenthetical/bracketed info
        name = re.sub(r'\s+', ' ', name).strip()
        
        parts = name.split()
        if len(parts) >= 2:
            return parts[0], parts[1]
        elif len(parts) == 1:
            return parts[0], None
        return None, None
    
    def extract_metadata_from_description(self, text):
        """Extract metadata from description text"""
        metadata = {}
        
        if not text:
            return metadata
        
        # Look for origin information
        origin_patterns = [
            r'from\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)',
            r'native\s+to\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)',
            r'endemic\s+to\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)'
        ]
        
        for pattern in origin_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                metadata['origin'] = match.group(1)
                break
        
        # Look for habitat information
        habitat_patterns = [
            r'(epiphyte|lithophyte|terrestrial)',
            r'(cloud forest|rainforest|mountain|elevation)'
        ]
        
        for pattern in habitat_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                metadata['habitat'] = match.group(1)
                break
        
        return metadata
    
    def save_orchid_record(self, orchid_data):
        """Save orchid record to database"""
        try:
            # Check if already exists
            existing = OrchidRecord.query.filter_by(
                scientific_name=orchid_data['scientific_name'],
                ingestion_source=orchid_data['ingestion_source']
            ).first()
            
            if existing:
                return existing
            
            # Create new record
            record = OrchidRecord(**orchid_data)
            db.session.add(record)
            db.session.commit()
            
            logger.info(f"‚úÖ Added {orchid_data['display_name']} from Andy's Orchids")
            return record
            
        except Exception as e:
            db.session.rollback()
            logger.error(f"Error saving orchid record: {e}")
            return None


class JaysInternetOrchidEncyclopedia:
    """Scraper for Jay's Internet Orchid Encyclopedia - comprehensive species database"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; OrchidBot/1.0; Educational/Research)'
        })
        self.base_url = "http://www.orchidspecies.com"
        
    def scrape_encyclopedia(self, max_genera=20):
        """Scrape Jay's comprehensive orchid encyclopedia"""
        logger.info(f"üå∫ Starting Jay's Internet Orchid Encyclopedia scraper")
        
        results = {'processed': 0, 'errors': 0, 'skipped': 0}
        
        # Major orchid genera covered in Jay's encyclopedia
        genera_pages = [
            'indexbulb.htm',  # Bulbophyllum
            'indexcatt.htm',  # Cattleya
            'indexdend.htm',  # Dendrobium
            'indexmasd.htm',  # Masdevallia
            'indexonci.htm',  # Oncidium
            'indexphal.htm',  # Phalaenopsis
            'indexpaph.htm',  # Paphiopedilum
            'indexvand.htm',  # Vanda
            'indexcymbidium.htm',  # Cymbidium
            'indexepidendrum.htm'  # Epidendrum
        ]
        
        processed_genera = 0
        for genus_page in genera_pages:
            if processed_genera >= max_genera:
                break
                
            try:
                genus_results = self.scrape_genus_index(genus_page)
                results['processed'] += genus_results['processed']
                results['errors'] += genus_results['errors']
                results['skipped'] += genus_results['skipped']
                
                processed_genera += 1
                time.sleep(3)  # Be respectful to the site
                
            except Exception as e:
                logger.error(f"‚ùå Error processing genus page {genus_page}: {e}")
                results['errors'] += 1
        
        return results
    
    def scrape_genus_index(self, genus_page):
        """Scrape a genus index page"""
        results = {'processed': 0, 'errors': 0, 'skipped': 0}
        
        try:
            url = f"{self.base_url}/{genus_page}"
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Find species links (Jay's uses specific link patterns)
            species_links = soup.find_all('a', href=re.compile(r'\.htm$'))
            
            # Filter for actual species pages
            species_links = [link for link in species_links 
                           if link.get('href') and not link.get('href').startswith('index')]
            
            logger.info(f"üìÇ Found {len(species_links)} species in {genus_page}")
            
            for link in species_links[:50]:  # Limit to prevent overwhelming
                try:
                    species_url = urljoin(self.base_url, link.get('href'))
                    orchid_data = self.scrape_species_page(species_url)
                    
                    if orchid_data:
                        self.save_orchid_record(orchid_data)
                        results['processed'] += 1
                    else:
                        results['skipped'] += 1
                        
                    time.sleep(1)  # Rate limiting
                    
                except Exception as e:
                    logger.error(f"Error processing species link: {e}")
                    results['errors'] += 1
                    
        except Exception as e:
            logger.error(f"Error scraping genus index {genus_page}: {e}")
            results['errors'] += 1
        
        return results
    
    def scrape_species_page(self, species_url):
        """Extract detailed species information"""
        try:
            response = self.session.get(species_url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract species name (usually in title or h1)
            title_elem = soup.find('title') or soup.find('h1')
            if not title_elem:
                return None
                
            full_name = title_elem.text.strip()
            
            # Parse scientific name
            genus, species = self.parse_scientific_name(full_name)
            if not genus:
                return None
            
            # Extract main image
            img_elem = soup.find('img', src=re.compile(r'\.(jpg|jpeg|png|gif)$', re.I))
            image_url = None
            if img_elem:
                image_url = urljoin(self.base_url, img_elem.get('src'))
            
            # Extract metadata from page content
            metadata = self.extract_comprehensive_metadata(soup)
            
            orchid_data = {
                'display_name': full_name,
                'scientific_name': full_name,
                'genus': genus,
                'species': species,
                'image_url': image_url,
                'region': metadata.get('distribution'),
                'native_habitat': metadata.get('habitat'),
                'cultural_notes': metadata.get('culture'),
                'photographer': metadata.get('photographer', 'Jay Pfahl'),
                'image_source': "Jay's Internet Orchid Encyclopedia",
                'ingestion_source': 'jays_encyclopedia_scraper'
            }
            
            return orchid_data
            
        except Exception as e:
            logger.error(f"Error scraping species page {species_url}: {e}")
            return None
    
    def parse_scientific_name(self, name):
        """Parse genus and species from orchid name"""
        # Jay's pages often have detailed titles, extract the core name
        name = re.sub(r'\s*-.*$', '', name)  # Remove everything after dash
        name = re.sub(r'\s*\|.*$', '', name)  # Remove everything after pipe
        name = re.sub(r'\s*\(.*?\)\s*', ' ', name)  # Remove parenthetical info
        
        parts = name.split()
        if len(parts) >= 2:
            return parts[0], parts[1]
        elif len(parts) == 1:
            return parts[0], None
        return None, None
    
    def extract_comprehensive_metadata(self, soup):
        """Extract comprehensive metadata from Jay's detailed pages"""
        metadata = {}
        
        page_text = soup.get_text()
        
        # Look for distribution/origin information
        distribution_patterns = [
            r'Distribution[:\s]+([^.]+)',
            r'Found\s+in\s+([^.]+)',
            r'Native\s+to\s+([^.]+)',
            r'Endemic\s+to\s+([^.]+)'
        ]
        
        for pattern in distribution_patterns:
            match = re.search(pattern, page_text, re.IGNORECASE)
            if match:
                metadata['distribution'] = match.group(1).strip()
                break
        
        # Look for habitat information
        habitat_patterns = [
            r'Habitat[:\s]+([^.]+)',
            r'grows?\s+(?:as\s+)?(?:an?\s+)?(epiphyte|lithophyte|terrestrial)',
            r'(cloud forest|rainforest|montane|lowland)'
        ]
        
        for pattern in habitat_patterns:
            match = re.search(pattern, page_text, re.IGNORECASE)
            if match:
                metadata['habitat'] = match.group(1).strip()
                break
        
        # Look for cultural information
        culture_patterns = [
            r'Culture[:\s]+([^.]{50,200})',
            r'Temperature[:\s]+([^.]+)',
            r'Light[:\s]+([^.]+)'
        ]
        
        for pattern in culture_patterns:
            match = re.search(pattern, page_text, re.IGNORECASE)
            if match:
                metadata['culture'] = match.group(1).strip()
                break
        
        # Look for photographer credit
        photo_patterns = [
            r'Photo\s+by\s+([^.]+)',
            r'Image\s+by\s+([^.]+)',
            r'¬©\s*([^.]+)'
        ]
        
        for pattern in photo_patterns:
            match = re.search(pattern, page_text, re.IGNORECASE)
            if match:
                metadata['photographer'] = match.group(1).strip()
                break
        
        return metadata
    
    def save_orchid_record(self, orchid_data):
        """Save orchid record to database"""
        try:
            # Check if already exists
            existing = OrchidRecord.query.filter_by(
                scientific_name=orchid_data['scientific_name'],
                ingestion_source=orchid_data['ingestion_source']
            ).first()
            
            if existing:
                return existing
            
            # Create new record
            record = OrchidRecord(**orchid_data)
            db.session.add(record)
            db.session.commit()
            
            logger.info(f"‚úÖ Added {orchid_data['display_name']} from Jay's Encyclopedia")
            return record
            
        except Exception as e:
            db.session.rollback()
            logger.error(f"Error saving orchid record: {e}")
            return None


class ImageMetadataExtractor:
    """Extract EXIF metadata from orchid images to get location and date information"""
    
    @staticmethod
    def extract_metadata_from_url(image_url):
        """Extract EXIF metadata from image URL"""
        try:
            response = requests.get(image_url, timeout=10)
            response.raise_for_status()
            
            image = Image.open(io.BytesIO(response.content))
            exifdata = image.getexif()
            
            metadata = {}
            
            # Extract basic EXIF data
            for tag_id in exifdata:
                tag = TAGS.get(tag_id, tag_id)
                data = exifdata.get(tag_id)
                
                if tag == "DateTime":
                    metadata['photo_date'] = data
                elif tag == "Artist":
                    metadata['photographer'] = data
                elif tag == "ImageDescription":
                    metadata['description'] = data
                elif tag == "Copyright":
                    metadata['copyright'] = data
            
            # Extract GPS data if available
            gps_info = exifdata.get_ifd(0x8825)  # GPS IFD
            if gps_info:
                gps_data = ImageMetadataExtractor.extract_gps_data(gps_info)
                metadata.update(gps_data)
            
            return metadata
            
        except Exception as e:
            logger.error(f"Error extracting metadata from {image_url}: {e}")
            return {}
    
    @staticmethod
    def extract_gps_data(gps_info):
        """Extract GPS coordinates from EXIF GPS data"""
        gps_data = {}
        
        try:
            for key in gps_info.keys():
                name = GPSTAGS.get(key, key)
                gps_data[name] = gps_info[key]
            
            # Convert coordinates to decimal degrees
            if 'GPSLatitude' in gps_data and 'GPSLongitude' in gps_data:
                lat = ImageMetadataExtractor.convert_to_degrees(gps_data['GPSLatitude'])
                lon = ImageMetadataExtractor.convert_to_degrees(gps_data['GPSLongitude'])
                
                # Apply direction
                if gps_data.get('GPSLatitudeRef') == 'S':
                    lat = -lat
                if gps_data.get('GPSLongitudeRef') == 'W':
                    lon = -lon
                
                return {
                    'latitude': lat,
                    'longitude': lon,
                    'location_source': 'EXIF_GPS'
                }
        
        except Exception as e:
            logger.error(f"Error processing GPS data: {e}")
        
        return {}
    
    @staticmethod
    def convert_to_degrees(value):
        """Convert GPS coordinates to decimal degrees"""
        d, m, s = value
        return d + (m / 60.0) + (s / 3600.0)


def run_advanced_scrapers():
    """Run all advanced scrapers"""
    from app import app
    
    with app.app_context():
        logger.info("üöÄ Starting advanced orchid scrapers...")
        
        total_results = {'processed': 0, 'errors': 0, 'skipped': 0}
        
        # Run Ecuagenera scraper
        logger.info("üá™üá® Starting Ecuagenera scraper...")
        ecuagenera = EcuageneraScraper()
        ecu_results = ecuagenera.scrape_orchid_catalog(max_pages=5)
        total_results['processed'] += ecu_results['processed']
        total_results['errors'] += ecu_results['errors']
        total_results['skipped'] += ecu_results['skipped']
        
        # Run Andy's Orchids scraper
        logger.info("üá∫üá∏ Starting Andy's Orchids scraper...")
        andys = AndysOrchidsScraper()
        andys_results = andys.scrape_orchid_catalog(max_pages=8)
        total_results['processed'] += andys_results['processed']
        total_results['errors'] += andys_results['errors']
        total_results['skipped'] += andys_results['skipped']
        
        # Run Jay's Encyclopedia scraper
        logger.info("üìö Starting Jay's Encyclopedia scraper...")
        jays = JaysInternetOrchidEncyclopedia()
        jays_results = jays.scrape_encyclopedia(max_genera=10)
        total_results['processed'] += jays_results['processed']
        total_results['errors'] += jays_results['errors']
        total_results['skipped'] += jays_results['skipped']
        
        logger.info(f"üéâ Advanced scraping complete!")
        logger.info(f"üìä Total processed: {total_results['processed']}")
        logger.info(f"‚ùå Total errors: {total_results['errors']}")
        logger.info(f"‚è≠Ô∏è  Total skipped: {total_results['skipped']}")
        
        return total_results


if __name__ == "__main__":
    run_advanced_scrapers()